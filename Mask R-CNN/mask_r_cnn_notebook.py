# -*- coding: utf-8 -*-
"""Mask R-CNN JNote (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16VrMh2zoF80edSpbqRh9FtYZw0wVapN6
"""

"""## Step 1: Install Detectron2"""
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# install open-cv package
#!pip install opencv-python

# install/update torch (PyTorch: open-source ML library) and torchvision 
#           (utils for working with compVision using PyTorch)
#!pip install -U torch torchvision

# install fvcore library (from Facebook compVision research projects) from Github
#!pip install git+https://github.com/facebookresearch/fvcore.git

# imported the installed torch & torchvision packages to python environment
import torch, torchvision
torch.cuda.empty_cache()

# print version of PyTorch installed
#torch.__version__

# clone Detectron2
# (object detection & segmentation library developed by Facebook AI research)
#!git clone https://github.com/facebookresearch/detectron2 detectron2_repo

# install its library in editable mode
#!pip install -e detectron2_repo
#!pip install -e detectron2

# ---------------------------
# Some basic setup
# Setup detectron2 logger:
# ---------------------------
# import detectron2 library & logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# ------------------------------
# import some common libraries
# ------------------------------
import matplotlib.pyplot as plt
import numpy as np
import cv2

# -----------------------------------------
# import some common detectron2 utilities
# -----------------------------------------
from detectron2.engine import DefaultPredictor
# import function to obtain config object:
#         contains various settings and options to configure detectron2 models
from detectron2.config import get_cfg
# import visualizer which is used to visualize predictions made by detectron2
from detectron2.utils.visualizer import Visualizer
# import classes used to register and access metadata and detectron2 models
from detectron2.data import MetadataCatalog, DatasetCatalog

"""## Step 2: Train on a Custom COCO Dataset

COCO(Common Objects in Context) is a popular dataset format used for oject detection, instance segmentation, and keypoint detection tasks.
"""

# import function that allows registration of COCO-format datasets
from detectron2.data.datasets import register_coco_instances

# register dataset
# register_coco_instances("dataset_name", metadata={}, "path/to/dataset.json", "path/to/images/")
register_coco_instances("zerowaste_train", {}, "/home/zerowaste/Mask R-CNN/Notebook and Dataset/zerowaste-f-final/splits_final_deblurred2/train/labels.json",
                        "/home/zerowaste/Mask R-CNN/Notebook and Dataset/zerowaste-f-final/splits_final_deblurred2/train/data/")

# retrieve metadata (classnames, attribute info, etc...)
#                           to understand structure and content of the dataset
zerowaste_metadata = MetadataCatalog.get("zerowaste_train")

# retrieve dataset dictionary which is info about individual samples
#          (image file paths, annotations, etc...) for training and evaluation
dataset_dicts = DatasetCatalog.get("zerowaste_train")

'''
# to verify data loading is correct
# printing a sample of our dataset with labels, focus on the boxes only
import random
import matplotlib.image as mpimg

# takes 3 random samples from the zerowaste_train dataset dataset_dicts list
for d in random.sample(dataset_dicts, 3):
    # load image
    #img = cv2.imread(d["file_name"])
    img = mpimg.imread(d["file_name"])

    # visualize image with annotations
    # --------------------------------------
    visualizer = Visualizer(img[:, :, ::-1], metadata = zerowaste_metadata, scale = 0.5)
                 # img[:, :, ::-1] to convert from BGR to RGB
                 # scale size of the displayed bounding boxes relative to the size of the image
                 # so scale = 0.5 means that the bounding boxes will be displayed at half the size of the original image
    vis = visualizer.draw_dataset_dict(d)
    #cv2_imshow(vis.get_image()[:, :, ::-1])
    plt.imshow(vis.get_image()[:, :, ::-1])
    plt.show()
'''

"""# Step 3: Fine-Tuning a COCO-Pretrained R50-FPN Mask R-CNN model on the zerowaste_train Dataset"""

from detectron2.data.datasets import register_coco_instances
register_coco_instances("zerowaste_val", {}, "/home/zerowaste/Mask R-CNN/Notebook and Dataset/zerowaste-f-final/splits_final_deblurred2/val/labels.json",
                        "/home/zerowaste/Mask R-CNN/Notebook and Dataset/zerowaste-f-final/splits_final_deblurred2/val/data/")

zerowaste_metadata_val = MetadataCatalog.get("zerowaste_val")
dataset_dicts_val = DatasetCatalog.get("zerowaste_val")

# import functions to use COCO evaluation metrics (AP, mAP, etc...)
#               to evaluate object detection and instance segmentation
from detectron2.evaluation import COCOEvaluator
from detectron2.evaluation import DatasetEvaluators

from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
#import os

# ---------------------
# configuration setup
# ---------------------
# initialise new configuration object
cfg = get_cfg()
cfg.set_new_allowed(True)
# load pre-defined config file for a Faster R-CNN model from the Detectron2 model zoo
cfg.merge_from_file("/home/zerowaste/Mask R-CNN/Notebook and Dataset/detectron2_repo/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")

# -----------------------
# dataset configuration
# -----------------------
# specify training dataset
cfg.DATASETS.TRAIN = ("zerowaste_train",)
# specify validation dataset
cfg.DATASETS.TEST = ("zerowaste_val",)   # no metrics implemented for this dataset
# multi-process data loading - two worker processes used for data loading
cfg.DATALOADER.NUM_WORKERS = 2

# ---------------------
# model configuration
# ---------------------
# set the initial weights of the model
cfg.MODEL.WEIGHTS = "/home/zerowaste/Mask R-CNN/Notebook and Dataset/zerowaste-f-final/r_cnn.pkl"  # Get it from the drive


# training configuration
# ------------------------
# number of images per batch
cfg.SOLVER.IMS_PER_BATCH = 4
# base learning rate
cfg.SOLVER.BASE_LR = 0.005
#cfg.SOLVER.LR_SCHEDULER_NAME = "WarmupMultiStepLR"
# maximum number of iterations
cfg.SOLVER.MAX_ITER = 4000   # 300 iterations seems good enough, but you can certainly train longer
# batch size per image
#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset
# our model has 4 classes
# cardboard, soft plastic, rigid plastic and metal
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4

# create the output dir specified in the config if it doesn't already exist
# this dir will be used to save logs and trained model checkpoints during training
os.makedirs(cfg.OUTPUT_DIR, exist_ok = True)

# training initialization and execution
#---------------------------------------
# initialize DefaultTrainer object
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume = False)
# start the training process
trainer.train()

"""# Step 4: Perform Inference with the Trained Model on the fruits_nuts Dataset.

## Step 4a: Create a Predictor Using the Model Trained
"""

from detectron2.data.datasets import register_coco_instances
register_coco_instances("zerowaste_test", {}, "/home/zerowaste/Mask R-CNN/Notebook and Dataset/zerowaste-f-final/splits_final_deblurred2/test/labels.json",
                        "/home/zerowaste/Mask R-CNN/Notebook and Dataset/zerowaste-f-final/splits_final_deblurred2/test/data/")

zerowaste_metadata_test = MetadataCatalog.get("zerowaste_test")
dataset_dicts_test = DatasetCatalog.get("zerowaste_test")

# test evaluation
from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader
from detectron2.evaluation import COCOEvaluator, inference_on_dataset

# set model weights
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")

#torch.load()
#========================================================================================================
# set threshold for test time inference
# objects with detection scores above this threshold will be considered as positive detections
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.80
# initialize predictor
predictor = DefaultPredictor(cfg)
# create COCO evaluator
evaluator = COCOEvaluator("zerowaste_test", cfg, False, output_dir="./output/")
# Build detection test loader to load images and annotations for evaluation
val_loader = build_detection_test_loader(cfg, "zerowaste_test")
# perform inference using the trained model on the test dataset
# evaluation metrics are computed and displayed
print(inference_on_dataset(trainer.model, val_loader, evaluator))

"""## Step 4b: Randomly Select Several Samples to Visualize the Prediction Results"""

#'''
import random
from detectron2.utils.visualizer import ColorMode

for d in random.sample(dataset_dicts, 3):
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)

    # Filter out boxes under 50% confidence level
    instances = outputs["instances"]
    scores = instances.scores
    mask = scores >= 0.5 #Change this accordingly or not
    instances = instances[mask]

    v = Visualizer(im[:, :, ::-1],
                   metadata=zerowaste_metadata_test,
                   scale=0.8,
                   #instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels
    )
    v = v.draw_instance_predictions(instances.to("cpu"))
    plt.imshow(v.get_image()[:, :, ::-1])
    plt.show()
#'''


"""## Step 5: Benchmark Inference Speed"""

import time
times = []

example_image_path = dataset_dicts_test[0]["file_name"]
im = cv2.imread(example_image_path)

for i in range(20):
    start_time = time.time()
    outputs = predictor(im)
    delta = time.time() - start_time
    times.append(delta)
mean_delta = np.array(times).mean()
fps = 1 / mean_delta
print("Average(sec):{:.2f},fps:{:.2f}".format(mean_delta, fps))
